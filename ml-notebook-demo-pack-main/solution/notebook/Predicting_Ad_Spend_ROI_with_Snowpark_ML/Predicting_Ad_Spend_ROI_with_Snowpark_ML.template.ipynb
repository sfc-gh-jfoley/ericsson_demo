{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06060903-e0b5-4970-be4d-799d5e7ba52f",
   "metadata": {
    "name": "project_overview",
    "collapsed": false
   },
   "source": "# Project Overview\n\nPerform data analysis and data preparation tasks to train a Linear Regression model to predict future ROI (Return On Investment) of variable ad spend budgets across multiple channels including search, video, social media, and email using Snowpark for Python and Snowflake ML.\n"
  },
  {
   "cell_type": "markdown",
   "id": "0cd6c2eb-6bf7-4ae8-902b-5816fda53e6b",
   "metadata": {
    "name": "data_eng_section",
    "collapsed": false
   },
   "source": "## Data Engineering -- Data Analysis and Data Preparation\n\n- Load data from Snowflake tables into Snowpark DataFrames\n- Perform Exploratory Data Analysis on Snowpark DataFrames\n- Pivot and Join data from multiple tables using Snowpark DataFrames\n- Demostrate how to automate data preparation using Snowflake Tasks"
  },
  {
   "cell_type": "markdown",
   "id": "75978241-4905-406c-a0c2-208bbcdbf2ad",
   "metadata": {
    "name": "step_1",
    "collapsed": false
   },
   "source": "## Import Libraries"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "import_libs",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "from snowflake.snowpark.session import Session\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.functions import month,year,col,sum\nfrom snowflake.snowpark.version import VERSION\nfrom snowflake.core import Root\nfrom snowflake.core.task import Task, StoredProcedureCall\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\nfrom snowflake.core import CreateMode\n\n# Misc\nfrom datetime import timedelta\nimport json\nimport logging \nlogger = logging.getLogger(\"snowflake.snowpark.session\")\nlogger.setLevel(logging.ERROR)\n\nsession = get_active_session()\n\n# Get current warehouse to use later\nresult = session.sql('SELECT CURRENT_WAREHOUSE()')\ncurrent_wh = result.to_pandas()[\"CURRENT_WAREHOUSE()\"][0]",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "acf35526-e031-44fe-8ced-5725a051f154",
   "metadata": {
    "name": "step_2",
    "collapsed": false
   },
   "source": "## Load Aggregated Campaign Spend Data from Snowflake table into Snowpark DataFrame\n\nLet's first load the campaign spend data. This table contains ad click data that has been aggregated to show daily spend across digital ad channels including search engines, social media, email and video.\n\nNote: Some other ways to load data in a Snowpark DataFrame\n\n- session.sql(\"select col1, col2... from tableName\")\n- session.read.options({\"field_delimiter\": \",\", \"skip_header\": 1}).schema(user_schema).csv(\"@mystage/testCSV.csv\")\n- session.read.parquet(\"@stageName/path/to/file\")\n- session.create_dataframe([1,2,3], schema=[\"col1\"])\n\nTIP: Learn more about [Snowpark DataFrames](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/dataframe)."
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "python",
    "name": "load_campaign_spend_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "snow_df_spend = session.table('campaign_spend')\nsnow_df_spend.queries",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "429243f4-d59a-43fd-b787-7bed1207729a",
   "metadata": {
    "name": "step_3",
    "collapsed": false
   },
   "source": "Actions like show(), collect(), count() send the DataFrame SQL for execution on the server\n\nNote: History object provides the query ID which can be helpful for debugging as well as the SQL query executed on the server."
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "show_query_history",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "with session.query_history() as history:\n    snow_df_spend.show(20)\nhistory.queries",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "388f6cae-b2ea-4db5-b948-62f745e5853d",
   "metadata": {
    "name": "step_4",
    "collapsed": false
   },
   "source": "## Total Spend per Year and Month For All Channels\n\nLet's transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions.\n\nTIP: For a full list of functions, refer to the [documentation](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/functions)."
  },
  {
   "cell_type": "code",
   "id": "dc22a0e7-83ca-470c-8430-60b5f9e1e4c1",
   "metadata": {
    "language": "python",
    "name": "calculate_stats",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Stats per Month per Channel\nsnow_df_spend_per_channel = snow_df_spend.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n    with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n\nsnow_df_spend_per_channel.show(10)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b9a17f7-f557-4321-b318-4068c98cc68e",
   "metadata": {
    "name": "step_5",
    "collapsed": false
   },
   "source": "## Pivot on Channel: Total Spend Across All Channels\n\nLet's further transform the campaign spend data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions. This transformation will enable us to join with the revenue table such that we will have our input features and target variable in a single table for model training.\n\nTIP: For a full list of functions, refer to the [documentation](https://docs.snowflake.com/en/developer-guide/snowpark/reference/python/latest/functions)."
  },
  {
   "cell_type": "code",
   "id": "813b07ec-cf73-44dd-bbf0-261268f73d8e",
   "metadata": {
    "language": "python",
    "name": "pivot_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "snow_df_spend_per_month = snow_df_spend_per_channel.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\nsnow_df_spend_per_month = snow_df_spend_per_month.select(\n    col(\"YEAR\"),\n    col(\"MONTH\"),\n    col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n    col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n    col(\"'video'\").as_(\"VIDEO\"),\n    col(\"'email'\").as_(\"EMAIL\")\n)\nsnow_df_spend_per_month.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16abb4e0-4a66-415b-99b0-a12863906fbb",
   "metadata": {
    "name": "step_6",
    "collapsed": false
   },
   "source": "## Save Transformed Data into Snowflake Table\n\nLet's save the transformed data into a Snowflake table SPEND_PER_MONTH."
  },
  {
   "cell_type": "code",
   "id": "bd9caea2-4825-4f86-bec7-80b3d9d07103",
   "metadata": {
    "language": "python",
    "name": "write_transformed_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snow_df_spend_per_month.write.mode('overwrite').save_as_table('SPEND_PER_MONTH')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b8a2e164-b2ba-44a0-9ace-fcb2503f27ac",
   "metadata": {
    "name": "step_7",
    "collapsed": false
   },
   "source": "## Automation: Run Campaign Spend Data Transformations As a Snowflake Task\n\nNote: Optionally you can run all these transformations as an automated task by deploying the code to Snowflake as a Snowpark Stored Procedure and executing it as a Snowflake Task.\n\nTIP: Learn more about [Stored Procedures](https://docs.snowflake.com/en/sql-reference/stored-procedures-python) and [Snowflake Tasks](https://docs.snowflake.com/en/sql-reference/sql/create-task)."
  },
  {
   "cell_type": "code",
   "id": "6f01b5c4-3f5b-4384-9051-e15ded939c48",
   "metadata": {
    "language": "python",
    "name": "define_data_pipeline",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def campaign_spend_data_pipeline(session: Session) -> str:\n  # DATA TRANSFORMATIONS\n  # Perform the following actions to transform the data\n\n  # Load the campaign spend data\n  snow_df_spend_t = session.table('campaign_spend')\n\n  # Transform the data so we can see total cost per year/month per channel using group_by() and agg() Snowpark DataFrame functions\n  snow_df_spend_per_channel_t = snow_df_spend_t.group_by(year('DATE'), month('DATE'),'CHANNEL').agg(sum('TOTAL_COST').as_('TOTAL_COST')).\\\n      with_column_renamed('\"YEAR(DATE)\"',\"YEAR\").with_column_renamed('\"MONTH(DATE)\"',\"MONTH\").sort('YEAR','MONTH')\n\n  # Transform the data so that each row will represent total cost across all channels per year/month using pivot() and sum() Snowpark DataFrame functions\n  snow_df_spend_per_month_t = snow_df_spend_per_channel_t.pivot('CHANNEL',['search_engine','social_media','video','email']).sum('TOTAL_COST').sort('YEAR','MONTH')\n  snow_df_spend_per_month_t = snow_df_spend_per_month_t.select(\n      col(\"YEAR\"),\n      col(\"MONTH\"),\n      col(\"'search_engine'\").as_(\"SEARCH_ENGINE\"),\n      col(\"'social_media'\").as_(\"SOCIAL_MEDIA\"),\n      col(\"'video'\").as_(\"VIDEO\"),\n      col(\"'email'\").as_(\"EMAIL\")\n  )\n\n  # Save transformed data\n  snow_df_spend_per_month_t.write.mode('overwrite').save_as_table('SPEND_PER_MONTH')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c7035a7-3609-402b-8da6-8104cb7b748e",
   "metadata": {
    "language": "python",
    "name": "register_task",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Register data pipeline function as a task\nroot = Root(session)\nmy_task = Task(name='campaign_spend_data_pipeline_task'\n               , definition=StoredProcedureCall(\n                   campaign_spend_data_pipeline, stage_location='@dash_sprocs'\n               )\n               , warehouse=f'''{current_wh}'''\n               , schedule=timedelta(minutes=3))\n\ntasks = root.databases[session.get_current_database()].schemas[session.get_current_schema()].tasks\ntask_res = tasks.create(my_task,mode=CreateMode.or_replace)\n\n# By default a Task is suspended and we need to resume it if we want it run based on the schema. Note that we can still execute a task by calling the execute method.\ntask_res.execute()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "155beaf3-16bd-489f-a1ce-538181dbf0b6",
   "metadata": {
    "name": "step_8",
    "collapsed": false
   },
   "source": "## Total Revenue per Year And Month\n\nNow let's load revenue table and transform the data into revenue per year/month using group_by() and agg() functions."
  },
  {
   "cell_type": "code",
   "id": "1d4578a9-3330-4e03-9b4f-d2eed87b052f",
   "metadata": {
    "language": "python",
    "name": "calculate_total_rev",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snow_df_revenue = session.table('monthly_revenue')\nsnow_df_revenue_per_month = snow_df_revenue.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\nsnow_df_revenue_per_month.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "630e468c-4781-4412-9f51-b894690d39e7",
   "metadata": {
    "name": "step_9",
    "collapsed": false
   },
   "source": "## Join Total Spend and Total Revenue per Year and Month Across All Channels\n\nNext let's join this revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training."
  },
  {
   "cell_type": "code",
   "id": "23705e14-a843-4b92-8b96-b14f9bdf2296",
   "metadata": {
    "language": "python",
    "name": "join_data",
    "collapsed": false
   },
   "outputs": [],
   "source": "snow_df_spend_and_revenue_per_month = snow_df_spend_per_month.join(snow_df_revenue_per_month, [\"YEAR\",\"MONTH\"])\nsnow_df_spend_and_revenue_per_month.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a19f2ef7-c6d7-4d73-9d31-987da24b0f36",
   "metadata": {
    "name": "step_10",
    "collapsed": false
   },
   "source": "## Examine Snowpark DataFrame Query and Execution Plan\n\nSnowpark makes is really convenient to look at the DataFrame query and execution plan using explain() Snowpark DataFrame function."
  },
  {
   "cell_type": "code",
   "id": "186edbc9-d878-4ec9-a03f-d694ab9b0494",
   "metadata": {
    "language": "python",
    "name": "show_execution_plan",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snow_df_spend_and_revenue_per_month.explain()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d18128c0-1c16-4710-be99-909329d418d3",
   "metadata": {
    "name": "step_11",
    "collapsed": false
   },
   "source": "## Save Transformed Data into Snowflake Table\n\nLet's save the transformed data into a Snowflake table SPEND_AND_REVENUE_PER_MONTH."
  },
  {
   "cell_type": "code",
   "id": "e8d3d424-ec65-4120-9dfc-0da0a21b5289",
   "metadata": {
    "language": "python",
    "name": "write_spend_rev_data",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "snow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('SPEND_AND_REVENUE_PER_MONTH')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37b86074-4332-4d6b-9314-44e93964a282",
   "metadata": {
    "name": "step_12",
    "collapsed": false
   },
   "source": "## Automation: Run Monthly Revenue Data Transformations As a Snowflake Task (DAG)\n\nNote: Optionally you can run all these transformations as an automated task by deploying the code to Snowflake as a Snowpark Stored Procedure and executing it as a Snowflake Task. By using a DAG we can run it AFTER campaign_spend_data_pipeline_task.\n\nTIP: Learn more about [Stored Procedures](https://docs.snowflake.com/en/sql-reference/stored-procedures-python) and [Snowflake Tasks](https://docs.snowflake.com/en/sql-reference/sql/create-task)."
  },
  {
   "cell_type": "code",
   "id": "db9ffcd9-ce7d-425a-86e4-21b42f9655cc",
   "metadata": {
    "language": "python",
    "name": "define_rev_data_pipeline",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def monthly_revenue_data_pipeline(session: Session) -> str:\n  # Load revenue table and transform the data into revenue per year/month using group_by and agg() functions\n  snow_df_spend_per_month_t = session.table('spend_per_month')\n  snow_df_revenue_t = session.table('monthly_revenue')\n  snow_df_revenue_per_month_t = snow_df_revenue_t.group_by('YEAR','MONTH').agg(sum('REVENUE')).sort('YEAR','MONTH').with_column_renamed('SUM(REVENUE)','REVENUE')\n\n  # Join revenue data with the transformed campaign spend data so that our input features (i.e. cost per channel) and target variable (i.e. revenue) can be loaded into a single table for model training\n  snow_df_spend_and_revenue_per_month_t = snow_df_spend_per_month_t.join(snow_df_revenue_per_month_t, [\"YEAR\",\"MONTH\"])\n\n  # SAVE in a new table for the next task\n  snow_df_spend_and_revenue_per_month_t.write.mode('overwrite').save_as_table('SPEND_AND_REVENUE_PER_MONTH')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dcb56324-fe90-47e5-bf8c-330da47e38cf",
   "metadata": {
    "name": "step_13",
    "collapsed": false
   },
   "source": "Note: Since monthly_revenue_data_pipeline is depened on that campaign_spend_data_pipeline is executed first we want to create a DAG to make sure they run in the correct order."
  },
  {
   "cell_type": "code",
   "id": "a6892443-055b-4cc3-9b12-5ea58a15f3d8",
   "metadata": {
    "language": "python",
    "name": "create_dag",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Delete the previous task\ntask_res.delete()\n\nwith DAG(\"de_pipeline_dag\", schedule=timedelta(minutes=3)) as dag:\n    # Create a task that runs our first pipleine\n    dag_spend_task = DAGTask(name='campaign_spend_data_pipeline_task'\n                        , definition=StoredProcedureCall(\n                                    campaign_spend_data_pipeline, stage_location='@dash_sprocs'\n                                )\n                        ,warehouse=f'''{current_wh}'''\n                        )\n    # Create a task that runs our second pipleine\n    dag_revenue_task = DAGTask(name='monthly_revenue_data_pipeline'\n                          , definition=StoredProcedureCall(\n                                monthly_revenue_data_pipeline, stage_location='@dash_sprocs'\n                            )\n                        ,warehouse=f'''{current_wh}'''\n                        )\n# Shift right and left operators can specify task relationships.\ndag_spend_task >> dag_revenue_task  # dag_spend_task is a predecessor of dag_revenue_task\n\nschema = root.databases[session.get_current_database()].schemas[session.get_current_schema()]\ndag_op = DAGOperation(schema)\n\ndag_op.deploy(dag,mode=CreateMode.or_replace)\n\n# A DAG is not suspended by default so we will suspend the root task that will suspend the full DAG\nroot_task = tasks[\"DE_PIPELINE_DAG\"]\nroot_task.suspend()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7aac0fb4-3a27-4800-bbd2-72a39fb264ea",
   "metadata": {
    "name": "step_14",
    "collapsed": false
   },
   "source": "## Run DAG\n\nNote that we can manually run DAGs even if they are suspended"
  },
  {
   "cell_type": "code",
   "id": "ab79fde1-28e2-41e2-a3bd-4377d392c13e",
   "metadata": {
    "language": "python",
    "name": "run_dag",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "dag_op.run(dag)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd0fa7d5-b026-43e4-aefd-d4ae259e8b23",
   "metadata": {
    "name": "step_15",
    "collapsed": false
   },
   "source": "## Resume DAG"
  },
  {
   "cell_type": "code",
   "id": "6d53b663-04bd-4f3f-9adb-7f722087c6b5",
   "metadata": {
    "language": "python",
    "name": "resume_dag"
   },
   "outputs": [],
   "source": "root_task = tasks[\"DE_PIPELINE_DAG\"]\nroot_task.resume()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f8d165ce-eb80-4e71-ac82-4618e8ab7f37",
   "metadata": {
    "name": "step_16",
    "collapsed": false
   },
   "source": "## Suspend Tasks\n\nNote: For the sake of this lab, if you resume the above tasks, suspend them to avoid unecessary resource utilization by executing the following commands."
  },
  {
   "cell_type": "code",
   "id": "dbde842a-562d-430f-9ded-44fd2c7d501d",
   "metadata": {
    "language": "python",
    "name": "suspend_task",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "root_task = tasks[\"DE_PIPELINE_DAG\"]\nroot_task.suspend()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebf928a5-3c4e-49b2-ab0b-1f406f9c1fa3",
   "metadata": {
    "name": "machine_learning_section",
    "collapsed": false
   },
   "source": "## Machine Learning\n\n- Load features and target from Snowflake table into Snowpark DataFrame\n- Prepare features for model training\n- Train ML model using Snowpark ML in Snowflake and upload the model to Snowflake stage\n- Register ML model and use it for inference from Snowflake Model Registry\n"
  },
  {
   "cell_type": "markdown",
   "id": "d18aad05-08c7-4790-b966-ff1c3cc7d527",
   "metadata": {
    "name": "step_17",
    "collapsed": false
   },
   "source": "## Import Libraries"
  },
  {
   "cell_type": "code",
   "id": "3bf7daf8-30cb-41d6-8b20-af69441a478c",
   "metadata": {
    "language": "python",
    "name": "import_ml_libs"
   },
   "outputs": [],
   "source": "# Snowpark for Python\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.version import VERSION\n\n# Snowpark ML\nfrom snowflake.ml.modeling.compose import ColumnTransformer\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.preprocessing import PolynomialFeatures, StandardScaler\nfrom snowflake.ml.modeling.linear_model import LinearRegression\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.version import VERSION as ml_version\n\n# Misc\n#import pandas as pd\nimport json\nimport logging \nlogger = logging.getLogger(\"snowflake.snowpark.session\")\nlogger.setLevel(logging.ERROR)\n\nsession = get_active_session()\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56077ff8-04a3-4378-b4e9-f6b1de2df459",
   "metadata": {
    "name": "step_18",
    "collapsed": false
   },
   "source": "## Features and Target\n\nAt this point we are ready to perform the following actions to save features and target for model training.\n\n- Delete rows with missing values\n- Exclude columns we don't need for modeling\n- Save features into a Snowflake table called MARKETING_BUDGETS_FEATURES"
  },
  {
   "cell_type": "code",
   "id": "778a2252-298f-40fa-9605-185e87885c4d",
   "metadata": {
    "language": "python",
    "name": "feature_engineering",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Load data\nsnow_df_spend_and_revenue_per_month = session.table('spend_and_revenue_per_month')\n\n# Delete rows with missing values\nsnow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.dropna()\n\n# Exclude columns we don't need for modeling\nsnow_df_spend_and_revenue_per_month = snow_df_spend_and_revenue_per_month.drop(['YEAR','MONTH'])\n\n# Save features into a Snowflake table call MARKETING_BUDGETS_FEATURES\nsnow_df_spend_and_revenue_per_month.write.mode('overwrite').save_as_table('MARKETING_BUDGETS_FEATURES')\nsnow_df_spend_and_revenue_per_month.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6cd7c377-012d-4eb6-a8e9-b04889e91942",
   "metadata": {
    "name": "step_19",
    "collapsed": false
   },
   "source": "## Model Training using Snowpark ML in Snowflake\n\nLearn more about [Snowpark ML](https://docs.snowflake.com/developer-guide/snowpark-ml/snowpark-ml-modeling).\n\nNOTE: For workloads that require a large amount of memory and compute resources, consider using [Snowpark-Optimized Warehouses](https://docs.snowflake.com/en/developer-guide/snowpark/python/python-snowpark-training-ml#snowpark-optimized-warehouses)."
  },
  {
   "cell_type": "code",
   "id": "23556230-7278-4115-a9ca-0984923b601b",
   "metadata": {
    "language": "python",
    "name": "model_training"
   },
   "outputs": [],
   "source": "CROSS_VALIDATION_FOLDS = 10\nPOLYNOMIAL_FEATURES_DEGREE = 2\n\n# Create train and test Snowpark DataDrames\ntrain_df, test_df = session.table(\"MARKETING_BUDGETS_FEATURES\").random_split(weights=[0.8, 0.2], seed=0)\n\n# Preprocess the Numeric columns\n# We apply PolynomialFeatures and StandardScaler preprocessing steps to the numeric columns\n# NOTE: High degrees can cause overfitting.\nnumeric_features = ['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL']\nnumeric_transformer = Pipeline(steps=[('poly',PolynomialFeatures(degree = POLYNOMIAL_FEATURES_DEGREE)),('scaler', StandardScaler())])\n\n# Combine the preprocessed step together using the Column Transformer module\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features)])\n\n# The next step is the integrate the features we just preprocessed with our Machine Learning algorithm to enable us to build a model\npipeline = Pipeline(steps=[('preprocessor', preprocessor),('classifier', LinearRegression())])\nparameteres = {}\n\n# Use GridSearch to find the best fitting model based on number_of_folds folds\nmodel = GridSearchCV(\n    estimator=pipeline,\n    param_grid=parameteres,\n    cv=CROSS_VALIDATION_FOLDS,\n    label_cols=[\"REVENUE\"],\n    output_cols=[\"PREDICTED_REVENUE\"],\n    verbose=2\n)\n\n# Fit and Score\nmodel.fit(train_df)\ntrain_r2_score = model.score(train_df)\ntest_r2_score = model.score(test_df)\n\n# R2 score on train and test datasets\nprint(f\"R2 score on Train : {train_r2_score}\")\nprint(f\"R2 score on Test  : {test_r2_score}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ccdd4bd-f6e2-4e08-80c5-353e4f0d7918",
   "metadata": {
    "name": "step_20",
    "collapsed": false
   },
   "source": "## Log Trained Model to Snowflake Model Registry\n\nThe Model Registry allows to store models as objects in a schema in Snowflake. Note that by default the database and schema of the session is used.\n\nLearn more about [Model Registry](https://docs.snowflake.com/developer-guide/snowpark-ml/model-registry/overview)."
  },
  {
   "cell_type": "code",
   "id": "6a244646-001a-4e66-8f9c-98d366041309",
   "metadata": {
    "language": "python",
    "name": "create_model_registry"
   },
   "outputs": [],
   "source": "registry = Registry(session)\nMODEL_NAME = \"PREDICT_ROI\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79e30019-b2db-4c57-9e21-0477b6c176c2",
   "metadata": {
    "language": "python",
    "name": "optional_step"
   },
   "outputs": [],
   "source": "# NOTE: If you try to log the model with the same name, you may get \"ValueError: (0000) Model PREDICT_ROI version v1 already existed.\" error. \n# If that's the case, uncomment and run this cell.\n\n# registry.delete_model(MODEL_NAME)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "abcc712c-ef5c-4bcf-b775-4523ba595e28",
   "metadata": {
    "language": "python",
    "name": "log_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "mv = registry.log_model(model,\n                        model_name=MODEL_NAME,\n                        version_name=\"v0\",\n                        metrics={\"R2_train\": train_r2_score, \"R2_test\":test_r2_score},\n                        comment='Model pipeline to predict revenue',\n                        options={\"embed_local_ml_library\": True,\n                                 \"relax_version\": False}\n                    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ce6c58e9-ad6d-4bfa-a942-e05b8ccfbb79",
   "metadata": {
    "language": "python",
    "name": "show_models",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "registry.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e99acf6e-bb91-488a-bcf3-f15e61a9a42d",
   "metadata": {
    "name": "step_21",
    "collapsed": false
   },
   "source": "## Inference\n\nOnce the model is logged we can use it for inference on new data.\n\nFirst we will create a Snowpark DataFrame with some sample data and then call the logged model to get new predictions. Note: we will handle negative values in our Streamlit application."
  },
  {
   "cell_type": "code",
   "id": "dfd16339-be09-4ec1-a9eb-57ffe3722ddb",
   "metadata": {
    "language": "python",
    "name": "execute_batch_inference",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "test_df = session.create_dataframe([[250000,250000,200000,450000],[500000,500000,500000,500000],[8500,9500,2000,500]], \n                                    schema=['SEARCH_ENGINE','SOCIAL_MEDIA','VIDEO','EMAIL'])\nmv.run(test_df, function_name='predict').show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ac2297a-8314-4a46-93be-38c314ccfcb8",
   "metadata": {
    "name": "step_22",
    "collapsed": false
   },
   "source": "## Let's do some clean up now."
  },
  {
   "cell_type": "code",
   "id": "08a6055a-9cea-442d-8123-f370c0b9a951",
   "metadata": {
    "language": "python",
    "name": "delete_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Clean up\nregistry.delete_model(MODEL_NAME)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4308096f-d805-4019-a05e-a19343632e9a",
   "metadata": {
    "language": "python",
    "name": "confirmed_deleted",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Confirm it was deleted\nregistry.show_models()",
   "execution_count": null
  }
 ]
}